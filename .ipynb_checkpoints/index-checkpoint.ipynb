{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f71d89",
   "metadata": {},
   "source": [
    "## Building a Natural Language Processing (NLP) Model that Rates the Sentiment of Tweets about Apple and Google Products as Positive, Negative or Neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41c5ff",
   "metadata": {},
   "source": [
    "+ **Student:** Wambui Munene\n",
    "+ **Student pace:** DSPT08\n",
    "+ **Scheduled project review date/time:** 12/02/2025 23.59 Hours\n",
    "+ **Instructor name:** Samuel Karu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf77a4",
   "metadata": {},
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafed66",
   "metadata": {},
   "source": [
    "### Business and Data Understanding\n",
    "\n",
    "The objective of this project is to build a Natural Language Processing (NLP) model that rates the sentiment of tweets about Apple and Google products as positive, negative or neutral. The dataset used to build the model is sourced from CrowdFlower via data.world https://data.world/crowdflower/brands-and-product-emotions. This dataset consists of slightly over 9,000 human-rated tweets.\n",
    "\n",
    "Sentiment Analysis is a powerful tool that provides businesses with deep insights into public perception of their products and services. By leveraging sentiment analysis, companies can effectively gauge customer sentiment and understand the emotional tone behind customer interactions. This enables businesses to identify  areas of concern in real-time, allowing them to proactively address customer needs and improve their offerings.\n",
    "\n",
    "Social media is a dynamic and widespread platform where customers freely express their thoughts and feelings about products, services, and brands.Using social media platforms like X (formerly twitter) to gauge sentiments is immensely valuable for businesses as it provides real-time and unfiltered insights into customer opinions and experiences. \n",
    "\n",
    "By analyzing these sentiments, companies can tap into a wealth of authentic feedback that traditional surveys or feedback forms might miss. This immediate access to customer sentiment enables businesses to swiftly identify trends, preferences, and potential issues, allowing for proactive engagement and timely adjustments to strategies.\n",
    "\n",
    "Additionally, sentiment analysis can be useful in understanding the broader market landscape, and how competitors are faring and tailor products to match or exceed market expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ce65f",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "Data preparation involved the following key steps that are critical for preparing text data for modeling:-\n",
    "1. **Dataset Loading and Cleaning:**\n",
    "- Loaded the dataset\n",
    "- Renamed column names and labels for better readability\n",
    "- Dealt with missing values\n",
    "\n",
    "2. **Data Splitting:**\n",
    " - Split the data into training, validation and test sets. The training set was set at 70% of the data while the validation and test sets will be 15% respectively. \n",
    " - The validation set was used to tune the hyperparameters and choose the best model configuration without overfitting the test data.\n",
    " \n",
    "3. **Text Transformation:** \n",
    "- Used Regular Expressions (REGEX) to remove irrelevant information such as URLs,mentions(@) and hastags(#).\n",
    "- Converted all text to lowercase to ensure uniformity\n",
    "- Applied lemmatization to reduce words to their base forms for consistent analysis and reducing complexity\n",
    "- Removed stop words (common words that typically do not carry significant meaning such as \"the,\" \"is,\" \"in,\" \"and,\" etc.). This helped in focusing on more meaningful words in the text, leading to better performance of NLP models.\n",
    "\n",
    "4. **Vectorization of Text Data:**\n",
    "- Transformed the cleaned text data into numerical representation (vectors) using Term Frequency-Inverse Document Frequency(TF-IDF). This technique evaluates the importance of a word in a document relative to a corpus.\n",
    "- Adjusted the ngram-range paramemter in the TF-IDF vectorizer to include both unigram(single words) bigram(pair of executive words) to capture context, enriching the feature set and enhancing the model performance.\n",
    "\n",
    "\n",
    "5. **Exploratory Data Analysis (EDA):**\n",
    " - Analyzed the distribution of sentiment labels (positive, negative,neutral) using bar charts and value counts to understand class balance.\n",
    " - Visualized the top 10 most common words in the data set.\n",
    " - Created word clouds for positive, negative and neutral tweets to visualize most common words in each sentimenclass\n",
    " - Visualized bigrams using bar charts to identify common word pairs in the data set, and for each sentiment class\n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of libraries used in the Data Preparation Process: \n",
    "\n",
    "# Regular Expressions (re): For cleaning text data\n",
    "import re \n",
    "\n",
    "# NLTK (Natural Language Toolkit): For tokenization, stop words removal, and lemmatization.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Scikit-learn: For TF-IDF vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Pandas, Matplotlib and Seaborn for data manipulation and analysis and visualizations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# WordCloud: For generating word clouds.\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5ae63",
   "metadata": {},
   "source": [
    "### 2. Modeling\n",
    "The project utilized a combination of baseline models and advanced neural networks on the cleaned and vectorized data.\n",
    "\n",
    "- Created pipelines to streamline data preprocessing (Normalizethe TF-IDF vectors using StandardScaler from Scikit-learn to ensure fair comparisons accross different features),model training and evaluation.This ensured a reproducible and efficient workflow, and minimized the risk of data leakage.\n",
    "- Initial models included Logistic Regression and Naive Bayes. These models were tuned using GridSearchCV, to find the best hyperparameters, and incorporated cross-validation to prevent overfitting. The optimal hyperparameters were then used on the validation set to fine-tune model performance.\n",
    "- For advanced modeling, Convolutional Neural Networks (CNNs) were implemented to capture local patterns within the text data. - - The accuracy of the CNN model were compared to those of the baseline models to evaluate their performance improvements.\n",
    "- After identifying the best-performing model, it was evaluated on the test set to provide an unbiased assessment of its generalization capability. The final test confirmed the model's robustness and accuracy in predicting sentiment on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ef0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for normalization.\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "#Scikit-learn for creating pipelines, training models, hyperparameter tuning, and evaluation.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# TensorFlow/Keras for building and training Convolutional Neural Networks (CNNs).\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba45928",
   "metadata": {},
   "source": [
    "### 3. Evaluation \n",
    "The model's performance was evaluated using the following metrics:\n",
    "\n",
    "- Accuracy: This metric was used to measure the overall correctness of the model's predictions. It represents the proportion of correct predictions out of the total number of predictions.\n",
    "\n",
    "- Confusion Matrix: This matrix was be used to provide a detailed breakdown of the model's performance by showing the counts of true positive, true negative, false positive, and false negative predictions. It helped to identify the types of errors the models were making and provided insights into its classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58a31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for calculating accuracy and generating the confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c2fe8",
   "metadata": {},
   "source": [
    "### 1.1 Load and Clean the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08cfd2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries for data analysis and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b064604c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "6                                                NaN   \n",
       "7  #SXSW is just starting, #CTIA is around the co...   \n",
       "8  Beautifully smart and simple idea RT @madebyma...   \n",
       "9  Counting down the days to #sxsw plus strong Ca...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "5                             NaN   \n",
       "6                             NaN   \n",
       "7                         Android   \n",
       "8              iPad or iPhone App   \n",
       "9                           Apple   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  \n",
       "5                 No emotion toward brand or product  \n",
       "6                 No emotion toward brand or product  \n",
       "7                                   Positive emotion  \n",
       "8                                   Positive emotion  \n",
       "9                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data as a DataFrame and display the first 10 columns\n",
    "df = pd.read_csv('tweet_product_company.csv', encoding='ISO-8859-1')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d4680",
   "metadata": {},
   "source": [
    "This data set consists of tweets mainly focussed on apple and google products showing positive, negative or neutral emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479b370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data set consists of 9093 rows\n",
      "This data set consists of 3 columns\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the data\n",
    "df.shape\n",
    "print(f\"This data set consists of {df.shape[0]} rows\")\n",
    "print(f\"This data set consists of {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0d2308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_text', 'emotion_in_tweet_is_directed_at',\n",
       "       'is_there_an_emotion_directed_at_a_brand_or_product'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa9f0a",
   "metadata": {},
   "source": [
    "The three columns are of the object data type; the names of the columns are rather wordy, so I will rename the column names to more user-friendly names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bbf8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "          sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename column names\n",
    "df.rename(columns={\n",
    "    'tweet_text': 'tweet',\n",
    "    'emotion_in_tweet_is_directed_at': 'product',\n",
    "    'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'\n",
    "}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c74074",
   "metadata": {},
   "source": [
    "The column names have been successfully renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff6398d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet      9092 non-null   object\n",
      " 1   product    3291 non-null   object\n",
      " 2   sentiment  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Get column attributes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6e747",
   "metadata": {},
   "source": [
    "There are significant null values under the product column accounting to more than 60% of the data set. I will first try to fill this column with either Apple or Google if the tweet contains the word iphone, ipad or google. Then fill all the remaining NAN values with 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a5ab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get value counts to see the distribution of products\n",
    "df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646e1de",
   "metadata": {},
   "source": [
    "The product distribution seems quite repetitive. All google products will be labelled Google while all Apple products(ipads/iphone) will be labelled Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc359835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet product         sentiment\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   Apple  Negative emotion\n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   Apple  Positive emotion\n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   Apple  Positive emotion\n",
       "3  @sxsw I hope this year's festival isn't as cra...   Apple  Negative emotion\n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  Google  Positive emotion"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to categorize products\n",
    "def categorize_product(tweet):\n",
    "    if pd.isnull(tweet):\n",
    "        return 'unknown'\n",
    "    tweet = tweet.lower()\n",
    "    if 'iphone' in tweet or 'ipad' in tweet or 'apple' in tweet:\n",
    "        return 'Apple'\n",
    "    elif 'google' in tweet or 'android' in tweet:\n",
    "        return 'Google'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply the function to the 'tweet' column and fill the 'product' column\n",
    "df['product'] = df['tweet'].apply(categorize_product)\n",
    "\n",
    "# Verify the changes\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65633bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Apple      5525\n",
       "Google     2781\n",
       "unknown     787\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d896065",
   "metadata": {},
   "source": [
    "This has immensely improved the product labeling. I will drop the unknown rows as they consist of less than 10% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd36556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Apple     5525\n",
       "Google    2781\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where the product is labeled 'unknown'\n",
    "df = df[df['product'] != 'unknown']\n",
    "\n",
    "# Verify the changes\n",
    "df['product'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fcded",
   "metadata": {},
   "source": [
    "The rows with unknown have been dropped. I will then check if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d38f3062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet        0\n",
       "product      0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13a16a",
   "metadata": {},
   "source": [
    "There are now no missing values in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0585f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "No emotion toward brand or product    4647\n",
       "Positive emotion                      2940\n",
       "Negative emotion                       569\n",
       "I can't tell                           150\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the value counts for the sentiment column\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb59a9",
   "metadata": {},
   "source": [
    "There are 4 labels in the sentiment column. The sentiment wordings are quite wordy, so I will change the wordings to Positive, Negative and Neutral and drop the rows where the sentiment is 'I can't tell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10266993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Neutral     4647\n",
       "Positive    2940\n",
       "Negative     569\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace sentiments\n",
    "df['sentiment'] = df['sentiment'].replace({\n",
    "    'No emotion toward brand or product': 'Neutral',\n",
    "    'Positive emotion': 'Positive',\n",
    "    'Negative emotion': 'Negative'\n",
    "})\n",
    "\n",
    "# Drop rows where sentiment is 'I can't tell'\n",
    "df = df[df['sentiment'] != \"I can't tell\"]\n",
    "\n",
    "# Verify the changes\n",
    "df['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e030f4d",
   "metadata": {},
   "source": [
    "The labels have been changed and the 'I can't tell' sentiment dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edce20",
   "metadata": {},
   "source": [
    "### 1.2 Data Splitting \n",
    "The data will be will split into the training, validation and test sets. The splits are 70% for training, 15% for validation, and 15% for testing. the validation set will be used to tune the hyperparameters and choose the best model configuration without overfitting the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5675620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5709,)\n",
      "X_val shape: (1223,)\n",
      "X_test shape: (1224,)\n",
      "y_train shape: (5709,)\n",
      "y_val shape: (1223,)\n",
      "y_test shape: (1224,)\n"
     ]
    }
   ],
   "source": [
    "# Import the relevant library from scikit-learn to split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# DeFine the features and target\n",
    "X = df['tweet']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ef0b0",
   "metadata": {},
   "source": [
    "### 1.3 Text Transformations\n",
    "In this section, we will start the process of preparing the feature column (tweet) for vectorization. This will involve:\n",
    "- removing unnessary text and symbols like URLs, mentions (@),hashtags(#),links,numbers,punctuation and symbols\n",
    "- convert all text to lowercase to ensure uniformity\n",
    "- apply lemmatization to reduce words to their base forms for consistent analysis and reducing complexity\n",
    "- remove stop words (common words that typically do not carry significant meaning such as \"the,\" \"is,\" \"in,\" \"and,\" etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18c2a560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6360    rt new iphone autocorrect already tried change...\n",
      "904     compiling sxsw list one google doc taking lot ...\n",
      "2285    read google circle rumor today sxsw thought le...\n",
      "5491    rt going check apple popup shop sxsw sxswi let...\n",
      "520     frankenstein amalgamation fragmented digital i...\n",
      "Name: tweet, dtype: object\n",
      "7378    sxsw photography iphone many live photo amp vi...\n",
      "5617    rt checkins fun need extend beyond fun make ch...\n",
      "3256    get tattoo free ipad heard mini cooper would l...\n",
      "2300    buy ipad makeshift sxsw apple store launch day...\n",
      "2443    google bing sitting panel look like want punch...\n",
      "Name: tweet, dtype: object\n",
      "1617         got new iphone app dialy grape sxsw thankyou\n",
      "3961    started using android app totally killer servi...\n",
      "2114    google quash circle rumour rt launching produc...\n",
      "6747                                    rt ipad take sxsw\n",
      "698     pic ipadwinning performance sxsw accordion too...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#', '', text)\n",
    "    # Remove [video] and {link}\n",
    "    text = re.sub(r'\\[.*?\\]|\\{.*?\\}', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Lemmatize text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_val = X_val.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)\n",
    "\n",
    "# Verify the changes\n",
    "print(X_train.head())\n",
    "print(X_val.head())\n",
    "print(X_test.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee016b23",
   "metadata": {},
   "source": [
    "The feature column has now been transformes into lowercase strings without numbers and symbols. Now we can vectorize the strings for modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302dfbb",
   "metadata": {},
   "source": [
    "### 1.4 Text Data Vectorization\n",
    "I will use Term Frequency=Inverse Document Frequency (TF-IDF) to transform text data into numerical features, capturing the importance of words and their combinations as unigrams(single words) and bigrams(pairs of consecutiver words).\n",
    "Using TF-IDF is important because it effectively weighs the significance of words in a document relative to the entire dataset, helping to distinguish relevant terms from common ones. Incorporating bigrams as features captures contextual information by considering pairs of consecutive words, enhancing the model's ability to understand and analyze the relationships between words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25a4ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5709, 35560)\n",
      "X_val shape: (1223, 35560)\n",
      "X_test shape: (1224, 35560)\n",
      "\n",
      "X_train: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X_val: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X_test: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Original datasets before vectorization\n",
    "# X_train_raw = X_train\n",
    "# X_val_raw = X_val\n",
    "# X_test_raw = X_test\n",
    "\n",
    "# # Initialize the TF-IDF vectorizer to include unigrams and bigrams\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# # Fit and transform the training data\n",
    "# X_train = vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "# # Transform the validation and test data\n",
    "# X_val = vectorizer.transform(X_val_raw)\n",
    "# X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "# # Verify the shape of the transformed data\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"X_val shape:\", X_val.shape)\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# print()  \n",
    "\n",
    "# # Display the first few rows of the dense arrays\n",
    "# print(\"X_train:\", X_train.toarray()[:5])\n",
    "# print(\"X_val:\", X_val.toarray()[:5])\n",
    "# print(\"X_test:\", X_test.toarray()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa764a3c",
   "metadata": {},
   "source": [
    "As expected, the features are quite large because of the inclusion of bigrams. I plan to apply Principle Component Analysis(PCA), a technique that transforms the original features into a new set of orthogonal components, capturing the most variance in the data. I will apply PCA after the baseline model will all the features to see if performance improves with the reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6c735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
