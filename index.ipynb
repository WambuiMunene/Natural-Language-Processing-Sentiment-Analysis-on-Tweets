{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f71d89",
   "metadata": {},
   "source": [
    "## Building a Natural Language Processing (NLP) Model that Rates the Sentiment of Tweets about Apple and Google Products as Positive, Negative or Neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41c5ff",
   "metadata": {},
   "source": [
    "+ **Student:** Wambui Munene\n",
    "+ **Student pace:** DSPT08\n",
    "+ **Scheduled project review date/time:** 12/02/2025 23.59 Hours\n",
    "+ **Instructor name:** Samuel Karu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf77a4",
   "metadata": {},
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafed66",
   "metadata": {},
   "source": [
    "### Business and Data Understanding\n",
    "\n",
    "The objective of this project is to build a Natural Language Processing (NLP) model that rates the sentiment of tweets about Apple and Google products as positive, negative or neutral. The dataset used to build the model is sourced from CrowdFlower via data.world https://data.world/crowdflower/brands-and-product-emotions. This dataset consists of over 8,000 human-rated tweets.\n",
    "\n",
    "Sentiment Analysis is a powerful tool that provides businesses with deep insights into public perception of their products and services. By leveraging sentiment analysis, companies can effectively gauge customer sentiment and understand the emotional tone behind customer interactions. This enables businesses to identify  areas of concern in real-time, allowing them to proactively address customer needs and improve their offerings.\n",
    "\n",
    "Social media is a dynamic and widespread platform where customers freely express their thoughts and feelings about products, services, and brands.Using social media platforms like X (formerly twitter) to gauge sentiments is immensely valuable for businesses as it provides real-time and unfiltered insights into customer opinions and experiences. \n",
    "\n",
    "By analyzing these sentiments, companies can tap into a wealth of authentic feedback that traditional surveys or feedback forms might miss. This immediate access to customer sentiment enables businesses to swiftly identify trends, preferences, and potential issues, allowing for proactive engagement and timely adjustments to strategies.\n",
    "\n",
    "Additionally, sentiment analysis can be useful in understanding the broader market landscape, and how competitors are faring and tailor products to match or exceed market expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ce65f",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Data preparation involved the following key steps that are critical for preparing text data for modeling:-\n",
    "1. **Data Cleaning:** \n",
    "- Used Regular Expressions (REGEX) to remove irrelevant information such as URLs,mentions(@) and hastags(#).\n",
    "- Converted all text to lowercase to ensure uniformity\n",
    "- Applied lemmatization to reduce words to their base forms for consistent analysis and reducing complexity\n",
    "- Removed stop words (common words that typically do not carry significant meaning such as \"the,\" \"is,\" \"in,\" \"and,\" etc.). This helped in focusing on more meaningful words in the text, leading to better performance of NLP models.\n",
    "\n",
    "2. **Feature Engineering:**\n",
    "- Transformed the cleaned text data into numerical representation (vectors) using Term Frequency-Inverse Document Frequency(TF-IDF). This technique evaluates the importance of a word in a document relative to a corpus.\n",
    "- Adjusted the ngram-range paramemter in the TF-IDF vectorizer to include both unigram(single words) bigram(pair of executive words) to capture context, enriching the feature set and enhancing the model performance.\n",
    "\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA):**\n",
    " - Analyzed the distribution of sentiment labels (positive, negative,neutral) using bar charts and value counts to understand class balance.\n",
    " - Visualized the top 10 most common words in the data set.\n",
    " - Created word clouds for positive, negative and neutral tweets to visualize most common words in each sentimenclass\n",
    " - Visualized bigrams using bar charts to identify common word pairs in the data set, and for each sentiment class\n",
    " \n",
    " 4. **Data Splitting:**\n",
    " - Split the data into training, validation and test sets. The training set was set at 70% of the data while the validation and test sets will be 15% respectively. \n",
    " - The validation set was used to tune the hyperparameters and choose the best model configuration without overfitting the test data.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726fbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of libraries used in the Data Preparation Process: \n",
    "\n",
    "# Regular Expressions (re): For cleaning text data\n",
    "import re \n",
    "\n",
    "# NLTK (Natural Language Toolkit): For tokenization, stop words removal, and lemmatization.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Scikit-learn: For TF-IDF vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Pandas, Matplotlib and Seaborn for data manipulation and analysis and visualizations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# WordCloud: For generating word clouds.\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5ae63",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "The project utilized a combination of baseline models and advanced neural networks on the cleaned and vectorized data.\n",
    "\n",
    "- Created pipelines to streamline data preprocessing (Normalizethe TF-IDF vectors using StandardScaler from Scikit-learn to ensure fair comparisons accross different features),model training and evaluation.This ensured a reproducible and efficient workflow, and minimized the risk of data leakage.\n",
    "- Initial models included Logistic Regression and Naive Bayes. These models were tuned using GridSearchCV, to find the best hyperparameters, and incorporated cross-validation to prevent overfitting. The optimal hyperparameters were then used on the validation set to fine-tune model performance.\n",
    "- For advanced modeling, Convolutional Neural Networks (CNNs) were implemented to capture local patterns within the text data. - - The accuracy of the CNN model were compared to those of the baseline models to evaluate their performance improvements.\n",
    "- After identifying the best-performing model, it was evaluated on the test set to provide an unbiased assessment of its generalization capability. The final test confirmed the model's robustness and accuracy in predicting sentiment on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381ef0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for normalization.\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "Scikit-learn for creating pipelines, training models, hyperparameter tuning, and evaluation.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# TensorFlow/Keras for building and training Convolutional Neural Networks (CNNs).\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba45928",
   "metadata": {},
   "source": [
    "### Evaluation Criteria\n",
    "The model's performance was evaluated using the following metrics:\n",
    "\n",
    "- Accuracy: This metric was used to measure the overall correctness of the model's predictions. It represents the proportion of correct predictions out of the total number of predictions.\n",
    "\n",
    "- Confusion Matrix: This matrix was be used to provide a detailed breakdown of the model's performance by showing the counts of true positive, true negative, false positive, and false negative predictions. It helped to identify the types of errors the models were making and provided insights into its classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for calculating accuracy and generating the con"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
