{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f71d89",
   "metadata": {},
   "source": [
    "## Building a Natural Language Processing (NLP) Model that Rates the Sentiment of Tweets about Apple and Google Products as Positive, Negative or Neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41c5ff",
   "metadata": {},
   "source": [
    "+ **Student:** Wambui Munene\n",
    "+ **Student pace:** DSPT08\n",
    "+ **Scheduled project review date/time:** 12/02/2025 23.59 Hours\n",
    "+ **Instructor name:** Samuel Karu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf77a4",
   "metadata": {},
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafed66",
   "metadata": {},
   "source": [
    "### Business and Data Understanding\n",
    "\n",
    "The objective of this project is to build a Natural Language Processing (NLP) model that rates the sentiment of tweets about Apple and Google products as positive, negative or neutral. The dataset used to build the model is sourced from CrowdFlower via data.world https://data.world/crowdflower/brands-and-product-emotions. This dataset consists of slightly over 9,000 human-rated tweets.\n",
    "\n",
    "Sentiment Analysis is a powerful tool that provides businesses with deep insights into public perception of their products and services. By leveraging sentiment analysis, companies can effectively gauge customer sentiment and understand the emotional tone behind customer interactions. This enables businesses to identify  areas of concern in real-time, allowing them to proactively address customer needs and improve their offerings.\n",
    "\n",
    "Social media is a dynamic and widespread platform where customers freely express their thoughts and feelings about products, services, and brands.Using social media platforms like X (formerly twitter) to gauge sentiments is immensely valuable for businesses as it provides real-time and unfiltered insights into customer opinions and experiences. \n",
    "\n",
    "By analyzing these sentiments, companies can tap into a wealth of authentic feedback that traditional surveys or feedback forms might miss. This immediate access to customer sentiment enables businesses to swiftly identify trends, preferences, and potential issues, allowing for proactive engagement and timely adjustments to strategies.\n",
    "\n",
    "Additionally, sentiment analysis can be useful in understanding the broader market landscape, and how competitors are faring and tailor products to match or exceed market expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ce65f",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "Data preparation involved the following key steps that are critical for preparing text data for modeling:-\n",
    "1. **Dataset Loading and Cleaning:**\n",
    "- Loaded the dataset\n",
    "- Renamed column names and labels for better readability\n",
    "- Dealt with missing values\n",
    "\n",
    "2. **Data Splitting:**\n",
    " - Split the data into training, validation and test sets. The training set was set at 70% of the data while the validation and test sets will be 15% respectively. \n",
    " - The validation set was used to tune the hyperparameters and choose the best model configuration without overfitting the test data.\n",
    " \n",
    "3. **Text Transformation:** \n",
    "- Used Regular Expressions (REGEX) to remove irrelevant information such as URLs,mentions(@) and hastags(#).\n",
    "- Converted all text to lowercase to ensure uniformity\n",
    "- Applied lemmatization to reduce words to their base forms for consistent analysis and reducing complexity\n",
    "- Removed stop words (common words that typically do not carry significant meaning such as \"the,\" \"is,\" \"in,\" \"and,\" etc.). This helped in focusing on more meaningful words in the text, leading to better performance of NLP models.\n",
    "- Tokenize the cleaned text\n",
    "\n",
    "4. **Vectorization of Text Data:**\n",
    "- Transformed the cleaned text data into numerical representation (vectors) using Term Frequency-Inverse Document Frequency(TF-IDF). This technique evaluates the importance of a word in a document relative to a corpus.\n",
    "- Adjusted the ngram-range paramemter in the TF-IDF vectorizer to include both unigram(single words) bigram(pair of executive words) to capture context, enriching the feature set and enhancing the model performance.\n",
    "\n",
    "\n",
    "5. **Exploratory Data Analysis (EDA):**\n",
    " - Analyzed the distribution of sentiment labels (positive, negative,neutral) using bar charts and value counts to understand class balance.\n",
    " - Visualized the top 10 most common words in the data set.\n",
    " - Created word clouds for positive, negative and neutral tweets to visualize most common words in each sentimenclass\n",
    " - Visualized bigrams using bar charts to identify common word pairs in the data set, and for each sentiment class\n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of libraries used in the Data Preparation Process: \n",
    "\n",
    "# Regular Expressions (re): For cleaning text data\n",
    "import re \n",
    "\n",
    "# NLTK (Natural Language Toolkit): For tokenization, stop words removal, and lemmatization.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Scikit-learn: For TF-IDF vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Pandas, Matplotlib and Seaborn for data manipulation and analysis and visualizations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# WordCloud: For generating word clouds.\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5ae63",
   "metadata": {},
   "source": [
    "### 2. Modeling\n",
    "The project utilized a combination of baseline models and advanced neural networks on the cleaned and vectorized data.\n",
    "\n",
    "- Created pipelines to streamline data preprocessing ,model training and evaluation. (Since we were working with sparse TF-IDF features, scaling was not necessary).This ensured a reproducible and efficient workflow, and minimized the risk of data leakage.\n",
    "- Initial models included Logistic Regression and Naive Bayes. These models were tuned using GridSearchCV, and PAC was used to reduce dimensionality to find the best hyperparameters, and incorporated cross-validation to prevent overfitting. The optimal hyperparameters were then used on the validation set to fine-tune model performance.\n",
    "- For advanced modeling, Convolutional Neural Networks (CNNs) were implemented to capture local patterns within the text data. - - The accuracy of the CNN model were compared to those of the baseline models to evaluate their performance improvements.\n",
    "- After identifying the best-performing model, it was evaluated on the test set to provide an unbiased assessment of its generalization capability. The final test confirmed the model's robustness and accuracy in predicting sentiment on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ef0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for normalization.\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "#Scikit-learn for creating pipelines, training models, hyperparameter tuning, and evaluation.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# TensorFlow/Keras for building and training Convolutional Neural Networks (CNNs).\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba45928",
   "metadata": {},
   "source": [
    "### 3. Evaluation \n",
    "The model's performance was evaluated using the following metrics:\n",
    "\n",
    "- Accuracy: This metric was used to measure the overall correctness of the model's predictions. It represents the proportion of correct predictions out of the total number of predictions.\n",
    "\n",
    "- Confusion Matrix: This matrix was be used to provide a detailed breakdown of the model's performance by showing the counts of true positive, true negative, false positive, and false negative predictions. It helped to identify the types of errors the models were making and provided insights into its classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for calculating accuracy and generating the confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c2fe8",
   "metadata": {},
   "source": [
    "### 1.1 Load and Clean the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfd2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries for data analysis and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data as a DataFrame and display the first 10 columns\n",
    "df = pd.read_csv('tweet_product_company.csv', encoding='ISO-8859-1')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d4680",
   "metadata": {},
   "source": [
    "This data set consists of tweets mainly focussed on apple and google products showing positive, negative or neutral emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "df.shape\n",
    "print(f\"This data set consists of {df.shape[0]} rows\")\n",
    "print(f\"This data set consists of {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa9f0a",
   "metadata": {},
   "source": [
    "The three columns are of the object data type; the names of the columns are rather wordy, so I will rename the column names to more user-friendly names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column names\n",
    "df.rename(columns={\n",
    "    'tweet_text': 'tweet',\n",
    "    'emotion_in_tweet_is_directed_at': 'product',\n",
    "    'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'\n",
    "}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c74074",
   "metadata": {},
   "source": [
    "The column names have been successfully renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6398d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column attributes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6e747",
   "metadata": {},
   "source": [
    "There are significant null values under the product column accounting to more than 60% of the data set. I will first try to fill this column with either Apple or Google if the tweet contains the word iphone, ipad or google. Then fill all the remaining NAN values with 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value counts to see the distribution of products\n",
    "df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646e1de",
   "metadata": {},
   "source": [
    "The product distribution seems quite repetitive. All google products will be labelled Google while all Apple products(ipads/iphone) will be labelled Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc359835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize products\n",
    "def categorize_product(tweet):\n",
    "    if pd.isnull(tweet):\n",
    "        return 'unknown'\n",
    "    tweet = tweet.lower()\n",
    "    if 'iphone' in tweet or 'ipad' in tweet or 'apple' in tweet:\n",
    "        return 'Apple'\n",
    "    elif 'google' in tweet or 'android' in tweet:\n",
    "        return 'Google'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply the function to the 'tweet' column and fill the 'product' column\n",
    "df['product'] = df['tweet'].apply(categorize_product)\n",
    "\n",
    "# Verify the changes\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65633bab",
   "metadata": {},
   "outputs": [],
   "source": [
    " df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d896065",
   "metadata": {},
   "source": [
    "This has immensely improved the product labeling. I will drop the unknown rows as they consist of less than 10% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd36556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the product is labeled 'unknown'\n",
    "df = df[df['product'] != 'unknown']\n",
    "\n",
    "# Verify the changes\n",
    "df['product'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fcded",
   "metadata": {},
   "source": [
    "The rows with unknown have been dropped. I will then check if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13a16a",
   "metadata": {},
   "source": [
    "There are now no missing values in the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0585f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the value counts for the sentiment column\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb59a9",
   "metadata": {},
   "source": [
    "There are 4 labels in the sentiment column. The sentiment wordings are quite wordy, so I will change the wordings to Positive, Negative and Neutral and drop the rows where the sentiment is 'I can't tell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10266993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace sentiments\n",
    "df['sentiment'] = df['sentiment'].replace({\n",
    "    'No emotion toward brand or product': 'Neutral',\n",
    "    'Positive emotion': 'Positive',\n",
    "    'Negative emotion': 'Negative'\n",
    "})\n",
    "\n",
    "# Drop rows where sentiment is 'I can't tell'\n",
    "df = df[df['sentiment'] != \"I can't tell\"]\n",
    "\n",
    "# Verify the changes\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db7cb4",
   "metadata": {},
   "source": [
    "The 'I can't tell' label has been dropped.The tweets are mostly neutral and positive with very few negative tweets, indicative of satisfaction with the products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edce20",
   "metadata": {},
   "source": [
    "### 1.2 Data Splitting \n",
    "The data will be will split into the training, validation and test sets. The splits are 70% for training, 15% for validation, and 15% for testing. the validation set will be used to tune the hyperparameters and choose the best model configuration without overfitting the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant library from scikit-learn to split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# DeFine the features and target\n",
    "X = df['tweet']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ef0b0",
   "metadata": {},
   "source": [
    "### 1.3 Text Transformations\n",
    "In this section, we will start the process of preparing the feature column (tweet) for vectorization. This will involve:\n",
    "- removing unnessary text and symbols like URLs, mentions (@),hashtags(#),links,numbers,punctuation and symbols\n",
    "- convert all text to lowercase to ensure uniformity\n",
    "- apply lemmatization to reduce words to their base forms for consistent analysis and reducing complexity\n",
    "- remove stop words (common words that typically do not carry significant meaning such as \"the,\" \"is,\" \"in,\" \"and,\" etc.). \n",
    "- tokenize the cleaned text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#', '', text)\n",
    "    # Remove [video] and {link}\n",
    "    text = re.sub(r'\\[.*?\\]|\\{.*?\\}', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove the word 'rt'\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    # Lemmatize text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "X_train_preprocessed = X_train.apply(preprocess_text)\n",
    "X_val_preprocessed = X_val.apply(preprocess_text)\n",
    "X_test_preprocessed = X_test.apply(preprocess_text)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Training Set:\")\n",
    "print(X_train_preprocessed.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(X_val_preprocessed.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(X_test_preprocessed.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307425d1",
   "metadata": {},
   "source": [
    "The feature column has now been transformed into lowercase strings without numbers and symbols. The next step is to tokenize the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874caaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create DataFrames to store the data\n",
    "train_df = pd.DataFrame({'tweet': X_train_preprocessed})\n",
    "val_df = pd.DataFrame({'tweet': X_val_preprocessed})\n",
    "test_df = pd.DataFrame({'tweet': X_test_preprocessed})\n",
    "\n",
    "# Define a function for tokenization\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Step 2: Create a new column for the tokenized text\n",
    "train_df['tweet_tokenized'] = train_df['tweet'].apply(tokenize_text)\n",
    "val_df['tweet_tokenized'] = val_df['tweet'].apply(tokenize_text)\n",
    "test_df['tweet_tokenized'] = test_df['tweet'].apply(tokenize_text)\n",
    "\n",
    "# Display the first 5 rows of each set\n",
    "print(\"Training Set - First 5 Rows:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Set - First 5 Rows:\")\n",
    "print(val_df.head())\n",
    "print(\"\\nTest Set - First 5 Rows:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Check the shapes of the DataFrames\n",
    "print(\"\\nTraining Set Shape:\", train_df.shape)\n",
    "print(\"Validation Set Shape:\", val_df.shape)\n",
    "print(\"Test Set Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee016b23",
   "metadata": {},
   "source": [
    "The tweets are now tokenized into individual words and a dataframe created with two columns - the preprocessed tweets and the tokenized tweets. The next step is to perform Exploratory Data Analysis (EDA) before vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf4a91",
   "metadata": {},
   "source": [
    "### 1.4 Exploratory Data Analysis (EDA) \n",
    "In this section I will: \n",
    "\n",
    "- Analyze Sentiment Distribution: Use bar charts and value counts to understand the class balance of sentiment labels (positive, negative, neutral). This helps identify any class imbalances.\n",
    "\n",
    "- Visualize Top Common Words: Identify and display the top 10 most common words in the dataset. This gives an overview of the predominant terms.\n",
    "\n",
    "- Create Word Clouds: Generate word clouds for positive, negative, and neutral tweets to visualize the most common words in each sentiment class. This provides a visual representation of word frequency and sentiment-specific terms.\n",
    "\n",
    "- Visualize Bigrams: Use bar charts to identify and display common word pairs (bigrams) in the dataset, and for each sentiment class. This reveals frequently occurring word combinations and their sentiment context.\n",
    "\n",
    "To perform EDA I will combine the train_df and the y_train (sentiments) into a single data frame. However I will use a copy   of the train_df to avoid modifying the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425449ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the number of rows are the same\n",
    "print(y_train.shape)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train_df and y_train\n",
    "train_eda_df = train_df.copy()\n",
    "train_eda_df['target'] = y_train.values\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "train_eda_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eda_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687bf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eda_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed911f17",
   "metadata": {},
   "source": [
    "#### 1.4.1 Analyze Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e39e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the sentiment labels\n",
    "sentiment_counts = train_eda_df['target'].value_counts()\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_counts.plot(kind='bar', color='blue')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd085f",
   "metadata": {},
   "source": [
    "The tweets are mostly neutral and positive with very few negative tweets, indicative of satisfaction with the products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc43e7c",
   "metadata": {},
   "source": [
    "#### 1.4.2 Visualize Top Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize the cleaned text data\n",
    "all_words = ' '.join(train_eda_df['tweet']).split()\n",
    "\n",
    "# Get the top 10 common words\n",
    "common_words = Counter(all_words).most_common(10)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "common_words_df = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
    "\n",
    "# Plot the top common words\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(common_words_df['Word'], common_words_df['Count'], color='blue')\n",
    "plt.title('Top 10 Common Words')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6000a",
   "metadata": {},
   "source": [
    "As is to be expected because the tweets are about Apple and Google products, the top words include ipad, apple, iphone,android etc meaning these words are common among tweets. The use of TF-IDF vectorization will put less weight on these words during modeling. However, I will visualize the top 10 words again excluding those common words. I will also exclude sxsw which is a reference to a conference founded in 1987 in Austin, Texas. SXSW® is best known for its conference and festivals that celebrate the convergence of tech, film, music, education, and culture.I will also exclude austin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78be3b",
   "metadata": {},
   "source": [
    "#### 1.4.3 Visualize Top Common Words Excluding Product Specific Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of words to exclude\n",
    "exclude_words = {'ipad', 'google', 'apple', 'iphone', 'app','android','sxsw','austin'}\n",
    "\n",
    "# Tokenize the cleaned text data\n",
    "all_words = ' '.join(train_eda_df['tweet']).split()\n",
    "\n",
    "# Remove the exclude words from the tokenized list\n",
    "filtered_words = [word for word in all_words if word not in exclude_words]\n",
    "\n",
    "# Get the top 10 common words\n",
    "common_words = Counter(filtered_words).most_common(10)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "common_words_df = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
    "\n",
    "# Plot the top common words excluding specified words\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(common_words_df['Word'], common_words_df['Count'], color='blue')\n",
    "plt.title('Top 10 Common Words (Excluding Specific Words)')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f240d",
   "metadata": {},
   "source": [
    "#### 1.4.4 Visualize Top Common Words by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589848a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to exclude\n",
    "exclude_words = {'ipad', 'google', 'apple', 'iphone', 'app', 'android', 'sxsw', 'austin'}\n",
    "\n",
    "# Function to count word frequencies excluding specific words\n",
    "def word_frequencies(data, sentiment, exclude_words, top_n=10):\n",
    "    text = ' '.join(data[data['target'] == sentiment]['tweet'])\n",
    "    words = [word for word in text.split() if word.lower() not in exclude_words]\n",
    "    counter = Counter(words)\n",
    "    common_words = counter.most_common(top_n)\n",
    "    return common_words\n",
    "\n",
    "# Plotting function for word frequencies in a single row\n",
    "def plot_word_frequencies_single_row(train_eda_df, sentiments, exclude_words, top_n=10):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(sentiments), figsize=(5*len(sentiments), 6))\n",
    "    for i, sentiment in enumerate(sentiments):\n",
    "        word_freq = word_frequencies(train_eda_df, sentiment, exclude_words, top_n)\n",
    "        words = [word[0] for word in word_freq]\n",
    "        frequencies = [word[1] for word in word_freq]\n",
    "        sns.barplot(y=frequencies, x=words, ax=axs[i])\n",
    "        axs[i].set_title(f'Top {top_n} Words in {sentiment} Tweets')\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        axs[i].set_xlabel('Word')\n",
    "        axs[i].tick_params(axis='x', rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sentiments to plot\n",
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "\n",
    "# Plot top ten words for each sentiment in a single row\n",
    "plot_word_frequencies_single_row(train_eda_df, sentiments, exclude_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0ca5c",
   "metadata": {},
   "source": [
    "From the charts above, the positive and neutral tweets have more words in common than with the negative tweets. Overall each sentiment has some distict words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74951e48",
   "metadata": {},
   "source": [
    "#### 1.4.5 Create Word Clouds by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a01483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# List of words to exclude\n",
    "exclude_words = {'ipad', 'google', 'apple', 'iphone', 'app', 'android', 'sxsw', 'austin'}\n",
    "\n",
    "# Function to generate word clouds\n",
    "def generate_wordcloud(data, sentiment, exclude_words):\n",
    "    text = ' '.join(data[data['target'] == sentiment]['tweet'])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=exclude_words).generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for {sentiment} tweets')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for each sentiment\n",
    "generate_wordcloud(train_eda_df, 'Positive', exclude_words)\n",
    "generate_wordcloud(train_eda_df, 'Negative', exclude_words)\n",
    "generate_wordcloud(train_eda_df, 'Neutral', exclude_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151632a",
   "metadata": {},
   "source": [
    "From the word cloud above you can see words like 'love', 'nice', 'great' in positive tweets and words like 'suck', 'fail' and 'headache' in negative tweets. The neutral tweets do not seem to have such strong words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6cae6",
   "metadata": {},
   "source": [
    "#### 1.4.6 Visualize Bigrams by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "# List of words to exclude\n",
    "exclude_words = {'ipad', 'google', 'apple', 'iphone', 'app', 'android', 'sxsw', 'austin'}\n",
    "\n",
    "# Function to generate bigram frequency\n",
    "def get_bigrams(data, sentiment, exclude_words, top_n=10):\n",
    "    tweets = ' '.join(data[data['target'] == sentiment]['tweet'])\n",
    "    tokens = [word for word in nltk.word_tokenize(tweets) if word.lower() not in exclude_words]\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigrams = finder.ngram_fd.items()\n",
    "    sorted_bigrams = sorted(bigrams, key=lambda item: item[1], reverse=True)[:top_n]\n",
    "    return sorted_bigrams\n",
    "\n",
    "# Plotting function for bigrams in a single row\n",
    "def plot_bigrams_single_row(train_eda_df, sentiments, exclude_words, top_n=10):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(sentiments), figsize=(5*len(sentiments), 6))\n",
    "    for i, sentiment in enumerate(sentiments):\n",
    "        bigrams = get_bigrams(train_eda_df, sentiment, exclude_words, top_n)\n",
    "        bigram_labels = [' '.join(bigram[0]) for bigram in bigrams]\n",
    "        frequencies = [bigram[1] for bigram in bigrams]\n",
    "        sns.barplot(y=frequencies, x=bigram_labels, ax=axs[i])\n",
    "        axs[i].set_title(f'Common Bigrams in {sentiment} Tweets')\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        axs[i].set_xlabel('Bigram')\n",
    "        axs[i].tick_params(axis='x', rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sentiments to plot\n",
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "\n",
    "# Visualize bigrams for each sentiment in a single row\n",
    "plot_bigrams_single_row(train_eda_df, sentiments, exclude_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8573691",
   "metadata": {},
   "source": [
    "The bigrams for positive and neutral tweets are quite similar. For the Negative tweets, even as they share common bigrams like new store and pop store which is expected as the tweets seem to be about a new launch, they have distinct strongly worded negative bigrams like 'design headache' and 'fascist company'.\n",
    "\n",
    "I will now go ahead and perform data vectorization using Term Frequency=Inverse Document Frequency (TF-IDF)to transform the text data into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302dfbb",
   "metadata": {},
   "source": [
    "### 1.5 Text Data Vectorization\n",
    "I will use Term Frequency=Inverse Document Frequency (TF-IDF) to transform text data into numerical features, capturing the importance of words and their combinations as unigrams(single words) and bigrams(pairs of consecutiver words).\n",
    "Using TF-IDF is important because it effectively weighs the significance of words in a document relative to the entire dataset, helping to distinguish relevant terms from common ones. This is especially important for this dataset as we have seen from the EDA process above that the tweets have a lot of words in common. Incorporating bigrams as features captures contextual information by considering pairs of consecutive words, enhancing the model's ability to understand and analyze the relationships between words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e783fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Function to join the list of tokens back into a single string\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Join tokens for each preprocessed set, applying it to each element\n",
    "X_train_joined = train_df['tweet_tokenized'].apply(join_tokens)\n",
    "X_val_joined = val_df['tweet_tokenized'].apply(join_tokens)\n",
    "X_test_joined = test_df['tweet_tokenized'].apply(join_tokens)\n",
    "\n",
    "# TF-IDF Vectorization for unigrams and bigrams\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Apply TF-IDF to the joined tokenized text\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(X_train_joined)\n",
    "val_tfidf = tfidf_vectorizer.transform(X_val_joined)\n",
    "test_tfidf = tfidf_vectorizer.transform(X_test_joined)\n",
    "\n",
    "# Verify the shape of the transformed data\n",
    "print(\"Training set shape:\", train_tfidf.shape)\n",
    "print(\"Validation set shape:\", val_tfidf.shape)\n",
    "print(\"Testing set shape:\", test_tfidf.shape)\n",
    "\n",
    "# Display the first few rows of the dense arrays\n",
    "print(\"Training set:\", train_tfidf.toarray()[:5])\n",
    "print(\"Validation set:\", val_tfidf.toarray()[:5])\n",
    "print(\"Testing set:\", test_tfidf.toarray()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa764a3c",
   "metadata": {},
   "source": [
    "As expected, the features are quite large because of the inclusion of bigrams. Each row corresponds to a document (a tweet), and each column represents a unique term (word or n-gram) from the entire corpus.\n",
    "\n",
    "The fact that the matrices are sparse (i.e., having many zeros) is expected and typical for TF-IDF representations, especially when using n-grams. Sparse matrices are well-suited for the machine learning models that I plan to use.\n",
    "\n",
    "I intend to use Principal Component Analysis (PCA), a method that converts the original features into a new set of orthogonal components, maximizing the variance captured in the data. I will apply PCA after evaluating the baseline model with all features to determine if performance improves with the reduced feature set.\n",
    "\n",
    "Now I will proceed to modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ef54b",
   "metadata": {},
   "source": [
    "### 2.0 Modeling\n",
    "- I will start with 2 baseline Logistic Regression and Naive Bayes Models\n",
    "- Use GridSearchCV and Dimensionality Reduction using PCA to optimize the best Baseline Model\n",
    "- Use Ensemble models - Random Forest, AdaBoost and XGBoost \n",
    "- Depending on the results obtained above train an Artificial Neural Network (ANN).\n",
    "- Evaluate each model iteration using Accuracy Score and Confusion Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e95b13",
   "metadata": {},
   "source": [
    "### 2.1 Baseline Models with Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04371854",
   "metadata": {},
   "source": [
    "#### 2.1.1 Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Create a pipeline with the estimator\n",
    "pipe_lr = Pipeline([\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced',random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_lr.fit(train_tfidf, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_pred_val_lr = pipe_lr.predict(val_tfidf)\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy_lr = accuracy_score(y_val, y_pred_val_lr)\n",
    "print(f\"Validation Accuracy (Logistic Regression): {accuracy_lr:.4f}\")\n",
    "\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.ylabel('Actual Label', fontsize=10)\n",
    "    plt.xlabel('Predicted Label', fontsize=10)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_lr = confusion_matrix(y_val, y_pred_val_lr)\n",
    "class_names = ['Positive', 'Neutral', 'Negative']\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(conf_matrix_lr, classes=class_names, title='Confusion Matrix - Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58133f88",
   "metadata": {},
   "source": [
    "The Accuracy Score is not that great at 67%. It is still better than random guessing, which for a 3-way class is 33.33%.\n",
    "- The model struggles significantly with correctly predicting Positive tweets.\n",
    "- The model performs relatively well with Neutral tweets \n",
    "- The model only has a moderate performance for Negative tweets.\n",
    "\n",
    "I will now train a baseline Naive Bayes model and evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76ac48",
   "metadata": {},
   "source": [
    "#### 2.1.2 Baseline Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a pipeline with the estimator\n",
    "pipe_nb = Pipeline([\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_nb.fit(train_tfidf, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_pred_val_nb = pipe_nb.predict(val_tfidf)\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy_nb = accuracy_score(y_val, y_pred_val_nb)\n",
    "print(f\"Validation Accuracy (Naive Bayes): {accuracy_nb:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_nb = confusion_matrix(y_val, y_pred_val_nb)\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(conf_matrix_nb, classes=class_names, title='Confusion Matrix - Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e80b7",
   "metadata": {},
   "source": [
    "The Accuracy Score of 63%, while still better than random guessingis even worse than that of the Logistic Model. \n",
    "- The model struggles significantly with correctly predicting Positive tweets.\n",
    "- The model performs relatively well with Neutral tweets \n",
    "- The model only has a moderate performance for Negative tweets.\n",
    "\n",
    "I will now use GridSearchCV and PCA to train the Logistic Regression Model as it appears to perform better than the Naive Bayes Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df3cd4",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter Tuned Logistic Regression Model with TruncatedSVD\n",
    "I have used TruncatedSVD instead of PCA after I encountered memory issues with PCA. TrunncatedSVD is designed to work with sparse matrices and it will still reduce dimensionality while avoiding the memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d666548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('svd', TruncatedSVD()),       # Truncated SVD for dimensionality reduction\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced',random_state=42))  # Logistic Regression classifier\n",
    "])\n",
    "\n",
    "# Define the hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'svd__n_components': [150,500,1000],  # Number of components for Truncated SVD\n",
    "    'clf__C': [1],              # Regularization strength for Logistic Regression\n",
    "    'clf__penalty': ['l2'],              # Regularization penalty (L2 is common for LR)\n",
    "    'clf__solver': ['liblinear'] # Solver to use in the optimization problem\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(pipe_lr, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "grid_search.fit(train_tfidf, y_train)\n",
    "\n",
    "# Best hyperparameters found by grid search\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate on the validation set using the best estimator\n",
    "y_pred_val_lr = grid_search.best_estimator_.predict(val_tfidf)\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy_lr = accuracy_score(y_val, y_pred_val_lr)\n",
    "print(f\"Validation Accuracy (Tuned Logistic Regression): {accuracy_lr:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_lr = confusion_matrix(y_val, y_pred_val_lr)\n",
    "# Plot the confusion matrix (assuming plot_confusion_matrix is a custom function)\n",
    "plot_confusion_matrix(conf_matrix_lr, classes=class_names, title='Confusion Matrix - Logistic Regression with Truncated SVD')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d323f55",
   "metadata": {},
   "source": [
    "While this model took eons to run, it did not have any significant improvement from the baseline models. I will try out ensemble models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af3dc6",
   "metadata": {},
   "source": [
    "### 2.3 Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9fa92",
   "metadata": {},
   "source": [
    "#### 2.3.1 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('clf', RandomForestClassifier(class_weight='balanced',random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200, 300],\n",
    "    'clf__max_depth': [None, 10, 20, 30],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=pipe_rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_pred_val = best_model.predict(val_tfidf)\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Validation Accuracy (Random Forest): {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_pred_val = best_model.predict(val_tfidf)\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Validation Accuracy (Random Forest): {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_pred_val)\n",
    "plot_confusion_matrix(conf_matrix, classes=class_names, title='Confusion Matrix - Random Forest')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
